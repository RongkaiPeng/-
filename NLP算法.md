# NLP算法

额外的隐藏层会提高准确度性能，但同时也会降低分类精度，因为会造成梯度消失导致后续层训练不足

![preview](https://pic1.zhimg.com/v2-40bb6d640b65f4aabaec56c47e46cbb4_720w.jpg?source=54b3c3a5)

#### RProp算法

1. 首先为各权重变化赋一个初始值，设定权重变化加速因子与减速因子。
2. 在网络前馈迭代中当连续误差梯度符号不变时，采用加速策略，加快训练速度；当连续误差梯度符号变化时，采用减速策略，以期稳定收敛。
3. 网络结合当前误差梯度符号与变化步长实现BP，同时，为了避免网络学习发生振荡或下溢，算法要求设定权重变化的上下限。

> 缺点：假设有一个在线学习系统，batch==1，初始的学习步长较小，在其上应用prop算法。这里有十组训练数据，前九组都使得梯度符号与之前的梯度符号相同，那么学习步长就会增加九次；而第十次得来的梯度符号与之前的相反，那么学习步长就会减小一次。这样一个过程下来，学习步长会增长很多（增大了9次学习步长，只减小了一次学习步长），如果系统的训练数据集非常之大，那学习步长可能频繁的来回波动，这样肯定是不利于学习的。  

#### RMSProp（RProp改进版算法）

rmsprop算法不再孤立地更新学习步长，而是联系之前的每一次梯度变化情况，具体如下。

1. rmsprop算法给每一个权值一个变量MeanSquare(w,t)用来记录第t次更新步长时前t次的梯度平方的平均值。
2. 然后再用第t次的梯度除上前t次的梯度的平方的平均值，得到学习步长的更新比例。
3. 根据此比例去得到新的学习步长。如果当前得到的梯度为负，那学习步长就会减小一点点；如果当前得到的梯度为正，那学习步长就会增大一点点。

这样看来，rmsprop算法步长的更新更加缓和。 
 这些算法并不能完全解决局部最小值问题，只是使得参数收敛的速度更快。针对是否能收敛到全局最优解，还与模型的初始化有关。