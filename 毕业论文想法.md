# 毕业论文想法

论文引用查询：semanticscholar.org

**中文词法分析？**：百度开放平台LAC

可以看一下的模型 FastNLP：https://fastnlp.readthedocs.io/zh/latest/index.html

**关于生成模型**：bart，GPT2.0（不开源）==MT5==

> bart: https://huggingface.co/transformers/model_doc/bart.html  
>
> mt5:https://zhuanlan.zhihu.com/p/302380842
>
> GPT2-chinese：https://github.com/imcaspar/gpt2-ml/blob/master/README_CN.md
>
> ​						   https://github.com/Morizeyao/GPT2-Chinese

**多标签文本分类：albert+seq2seq+Attention**:https://zhuanlan.zhihu.com/p/260743336

什么是generation？

人类从小开始，从字到词，example：我想要吃饭，对于小孩子来说，只有饭，而后慢慢有了吃这个动作，之后学会了我，然后有想要，对我们来说，没有主谓宾，没有动词名词，没有副词，我们总是自然而然地想到去说出以一个**单词**为主体的一句话，那么对于机器来说，只要塞给他们足够的数据，足够日常的对话，从向量矩阵的关系里面，他们是不是自然而然的就能理解如何去以核心词为中心，构建句式，比如时间、名词、主语等作为主体，计算机会根据他所理解的排列方式，加上其他足够相关的词，来组成自己的对话

而这些对话数据，我们可以通过，siri，小ai，小度等的人类的对话信息转文字来进行训练，这是第一手的数据，还有人们的评论信息（在我眼里他是第二手，人们会去考虑措辞）

## 疑问：

对于bert来说，仅仅是前后句子标记为0 1 是不是不合理？一整篇文章，同一句话之间内容相关联度较高，但不同句（比如第二句、第四句之间）也会存在关联度，是不是应该在segEmbedding、posEmbedding之外，再加一个整篇文章句子的编码，第一句、第二句、第三句、相隔较远的句子关联度较低，较近的句子关联度高（==甚至可以分段落？==）

## 数数数数数据集

自动摘要数据集：https://blog.csdn.net/weixin_43685844/article/details/99851563\

摘要数据集：清华大学 http://nlpr-web.ia.ac.cn/cip/dataset.htm

复述、同义语料库：https://github.com/cipnlu/Chinese-PPDB

中文摘要数据集：LCSTS

大规模中文自然语言处理语料 https://github.com/brightmart/nlp_chinese_corpus

爬虫软件（文本挖掘预处理）：https://github.com/ViDA-NYU/ache

​					https://www.crummy.com/software/BeautifulSoup/

天池数据集：https://tianchi.aliyun.com/dataset/

DF数据集:https://www.datafountain.cn/dataSets?category=3&page=4

## 爬虫

## 数据标注

https://blog.csdn.net/weixin_43685844/article/details/99851563

基于这两天的总结，我们大致可以选用Brat，YEDDA，Prodigy、snorkel试一下，选用Brat是因为有现成的中文情感标注，选用YEDDA是它完全用Python桌面开发且支持中文，选用Prodigy是因为支持标注的功能最全

https://blog.csdn.net/sinat_26917383/article/details/54908389

## 分词

==新词的学习==（为止语言现象处理）：一般来说，对于一个成熟的系统，新词会划分为错误词（or 未登录词），这时给每个错误词（未登录词）一个标记，这个标记是指数型增长的，在增长到一定值之后，就默认这个词是一个完整的词了，比如sigmoid函数可以用来这样，初期增长小，出现次数多了之后，就会有一个大增长，而后，到达一个理想值（不一定是1，可以是0.75这样的值，就默认他存为新词，清空标记）

 <img src="https://bkimg.cdn.bcebos.com/pic/c9fcc3cec3fdfc03f23fbf16d73f8794a5c226dc?x-bce-process=image/resize,m_lfit,w_500,limit_1" alt="Sigmoid 曲线" style="zoom:50%;" /> 

==***==能不能用transformer做分词呢？注意力会将中文中字的相关性连接起来，这样的话对于有歧义的字词，我们可以通过上下文字词的关联来考虑它应该如何分词

在特定领域的分词中，尤其是计算机方面、同义词、缩略词有很多，在预训练的基础上，加上计算机、安全等的文章、名词进行微调，掌握字和字之间的相关性，在文章中分词时，可以根据第一个字出现后，紧接着出现的其他关联度较高的字的出现，来判断是不是属于同一个词（由此来判断缩略词）

中文分词技术是中文文本分类中特有的概念，由于英文单词之间有空格将各个词分隔开，而中文则是根据词和词之间的概念来区分，在文本中并没有显式的分割界限，正因为这样，中文分词是中文文本分类预处理中关键的一步。在中文文本中，一般认为选取词作为特征项要优于字和词组，这是因为单字所代表的信息量太少，而且存在很多多义字，字与字之间的界限模糊，而且用字作为特征项将导致特征空间的高维灾难;词组虽然携带足够的信息量，但词组在文本中出现的机率不多，选用词组来作为特征项，会导致特征向量稀少，损失很多重要信息。
    分词目的是将连续的字序列按照一定的规范重新组合成词序列的过程。目前的分词算法一般分为以下三类

==命名实体识别==：英文安全类数据集：https://github.com/stucco

## 实体命名识别

BERT-CRF

Bert+CRF和Bert+LSTM+CRF进行NER识别的任务，结果表明加CRF比不加CRF结果有比较明显提升，但加LSTM和不加LSTM结果并没有明显区别

​					http://www.c-s-a.org.cn/html/2020/6/7269.html

实体词识别（在推荐系统里也同样适用，生成标签），要先进行分词&词性标注，抽取候选词，去歧义词、计算相关性

哈工大语言技术平台 http://www.ltp-cloud.com/intro

bert ner   https://github.com/kyzhouhzau/BERT-NER

### bert字词特征联合提取

![img](https://pic4.zhimg.com/80/v2-033f184f1f19ef97d92cf1469375712b_720w.jpg)

## 关键词抽取

### 1.关键词一次提取

#### 词嵌入算法

1.Embedding layer

将单词转化为one-hot形式，进而可以变成神经网络中的input形式，向量输入

2.word2vec google提出，学习的单词向量可以携带语义信息

3.Glove 对word2vec的拓展，在全文检索统计方面很好，但不如word2vec可以捕获语义



有很多开源的词嵌入向量模型

==是不是可以考虑自己利用爬取的数据去训练一个词嵌入向量模型==

或者把模型和nlp模型一起训练，==但这样的话，词嵌入模型就只适合这一个nlp任务==



### 2.关键词二次组合



## 文章摘要（可以考虑做一个关键词的突出显示）

目前的一些技术的小总结：https://blog.csdn.net/weikai_w/article/details/103679836

可以用来训练测试的数据集：LCSTS，NLPCC2017

中文词汇量院小于英文词汇量，但也存在为登录词的问题，未登录词可以

选用nlpcc摘要来进行评测任务，使用Rouge来作为评价指标，对摘要结果进行评价，Rouge主要用于计算模型产生的文摘与标准文摘之间的一元词、二元词即最长公共子串等的重叠率

在文章摘要这种下游任务中，实际生成的内容都是==自左向右==的，所以，ELMO、LSTM可以被纳入考虑，这些模型天然匹配这种过程



而bert这种DAE模式，在生成类NLP任务中，训练过程和应用过程不一致，所以生成类做的不是很好  

GPT的中文实现：     https://github.com/zingp/pointer-generator-pytorch      

GPT的源码：https://github.com/abisee/pointer-generator

可以用新闻标题做参考摘要，用自回归的预训练模型，如XLNET或者GPT2做fineTuning，或者用bottlesum的做法

可以考虑bertsum的方法，presum貌似也可以，英文的模型对于中文来说，训练是个大问题（时间、数据集）

==摘要效果应该怎么测评呢？==：Neural Headline Generation with Minimum Risk Training 这篇论文主要新颖点在于将ROUGE、BLEU等无法直接用作损失函数的评测指标（因为这些指标都不可微，不能参与梯度更新）引入到了训练目标函数中：（论文中的摘要子集就是随机采样）

## 构建图谱S





## data-to-text（结构化数据生成文本）



## 信息抽取

信息抽取旨在从非结构化自然语言文本中提取结构化知识，如实体、关系、事件等。

#### 事件抽取方案：

模式匹配的元事件抽取：ExDisco、GenPAM







## 评价指标：

data-to-text评价指标：

data to text 和翻译、摘要等生成式任务最大的不同是，input是类似于table或者三元组等其他形式的数据。在评估生成结果时，我们还需要考虑文本是否准确的涵盖了data的信息。

### relation generation (RG)

Relation generation 指从生成的句子中抽取出关系，然后对比有多少关系也出现在了source中（一般有recall和count2个指标）;

>  This measures how well the system is able to generate text containing factual (i.e., correct) records.

### content selection (CS)

content selection 一般指data当中的内容有多少出现在了生成的句子中，一般有precision和recall两个指标；

>  This measures how well the generated document matches the gold document in terms of selecting which records to generate.

### content ordering (CO)

content ordering 使用归一化 Damerau-Levenshtein距离计算生成句和参考句的“sequence of records(个人认为可以理解为item)”

>  This measures how well the system orders the records it chooses to discuss.

### 如何实现上述的评价指标

具体到某一个任务而言（这里以ROTOWIRE数据集，篮球比赛报道为例），ROTOWIRE数据以(entity , value , type)，例如(MIAMI HEAT,95, POINTS)的形式出现。针对以下参考句：

>  The Atlanta Hawks defeated the Miami Heat , 103 - 95 , at Philips Arena on Wednesday .

首先抽取出现在文本当中的(实体-值)对(例如“迈阿密热火”-95)，然后预测该对的type值是什么（正确答案应该是POINTS）。作者提到，许多信息抽取系统都是通过这样的方式把问题从多关系抽取简化为多目标分类问题，以此train一个简单的信息抽取的model。

若如此做，在test阶段就拥有一个抽取+分类的model用以评估上述指标。作者指出，他们的model拥有90%的精确率和60%的召回率，这足以支持他们来评估自己生成句子的质量。

具体实现和代码可以参照： [https://github.com/harvardnlp/data2text](https://link.zhihu.com/?target=https%3A//github.com/harvardnlp/data2text)



### Coverage

如果你的data to text不涉及复杂的关系抽取，也可以简单的通过匹配方法来验证文本是否能够覆盖要描述的data。

>  This metric measures the average proportion of input items that are  covered by a generated text. We recognized attribute values  (ingredients) with string match heuristics.

### Distinct

在某些生成场景中（对话，广告文案）等，还需要追求文本的多样性。李纪为的《A diversity-promoting objective function for neural conversation  models》提出了Distinct指标，后续也被许多人采用。

Distinct的定义如下：

![img](https://pic4.zhimg.com/80/v2-f33878be0bd16ed6cbdf1d601e169bc3_720w.jpg)

 Count(unique ngram)表示回复中不重复的ngram数量,Count(word)表示回复中ngram词语的总数量。

Distinct-n越大表示生成的多样性越高。