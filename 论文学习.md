# 论文学习

北京邮电大学网络知网库：https://libcon.bupt.edu.cn/login

https://arxiv.org/list/cs.CL/recent

人工智能领域论文https://paperswithcode.com/

要看的卢论文：

Text Generation from Knowledge Graphs with Graph Transformers. NAACL 2019》

> 从长文本构建KG，而后辅助生成文本
>
> 里面有KG的专业信息抽取工具
>
> 



### 1. Dependency-based Gated Recursive Neural Network for Chinese Word Segmentation

> 解决长距离依赖问题，将局部特征和长距离依赖问题结合
>
> 基于依赖的门控循环神经网络

### 2.Attention is all you need (Transformer机制开始)

> 内容来源于论文原文和哔哩哔哩中野生技术协会转载自bing.com的视频：从中文transformer到bert的模型精讲 ，url：https://www.bilibili.com/video/BV1sE411Y7cP?from=search&seid=10327299529714344083

#### 1.positional encoding位置嵌入 

transformer模型没有循环神经网络的迭代操作，必须给每个字的位置一个标识，提供给模型，以便于识别==顺序关系==

采用sin,cos对奇偶位置进行编码，以产生独特的位置信息，进行位置编码 



#### 2.self atention mechanism自注意力机制

对于X~embedding~=EmbeddingLookup(X)+PositionalEncoding   

K,Q,V矩阵维度和之前的线性变换得维度一致，W~q~ W~k~ W~v~ 是权重矩阵

Q = linear（X~embedding~）=X~embedding~ W~Q~

==multi head attention==多头注意力机制，引入超参数，number of heads（头数）

要求，嵌入维度（Embedding dim.）要能整除头数，进行平等的分割，将KQV进行分割，，分割后为了方便运算进行转置，由此，kQV的维度为[batch size, h, sequence length, embedding dimension/h]

Q点乘K^T^ ，如下图

<img src="C:\Users\q1171\AppData\Roaming\Typora\typora-user-images\image-20200907093954013.png" alt="image-20200907093954013" style="zoom: 67%;" />

> 每一行代表每个字和该句中其他各字符的相关性

由此，词向量（c1等）越相似，对应的结果矩阵的对应点乘结果就越大，就成了自注意力矩阵，代表词与词的关联程度（为了表示概率，就需要归一化）
$$
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt d_k})\cdot V
$$

$\sqrt d_k$是为了$Q\cdot K^T$之后矩阵被放大，除以$\sqrt d_k$可以使矩阵缩放，Q，K都是均值为0，方差为1，而点积之后，矩阵为均值0，方差d~K~

<img src="C:\Users\q1171\AppData\Roaming\Typora\typora-user-images\image-20200907102444908.png" alt="image-20200907102444908" style="zoom:67%;" />

让所有字的信息，融入到当前字里面来

因此attention机制的想法就是让现存的每一个字都含有句中其他所有字的信息

进行selfattention之后，V的维度并没有变化

>  softmax ,将数值转换为概率，

<img src="C:\Users\q1171\AppData\Roaming\Typora\typora-user-images\image-20200907103343117.png" alt="image-20200907103343117" style="zoom:67%;" />


> mask 解决输入向量不定长问题，需要将短句补成和最大句长进行计算，用0填充，这个过程叫做padding
>
> 但padding的过程对softmax有影响，因此需要给超出句长的无效区域一个很大的负数偏置



#### 3.Add & Norm

3.1残差连接

Attention（Q，K，V）和X~enbedding~ 维度一致，将他们直接相加，做残差连接，这样训练的时候可以使梯度走捷径（跨层）直接反传到初始层，避免了梯度消失的问题

3.2 LayerNorm

将神经网络中的隐藏层归一为标准正态分布，以起到加快训练速度，加速收敛的作用

> 将其参数统一到一个球上边，加速收敛
>
> <img src="C:\Users\q1171\AppData\Roaming\Typora\typora-user-images\image-20200907104850386.png" alt="image-20200907104850386" style="zoom:50%;" />
>
> 右下图进行归一化之后训练收敛明显比坐下快，这就是归一化的作用

<img src="C:\Users\q1171\AppData\Roaming\Typora\typora-user-images\image-20200907104435823.png" alt="image-20200907104435823" style="zoom:67%;" />

 这其中的参数$\alpha$和$\beta$都是可学习的，通常初始化$\alpha$$为全1，$$\beta$为全0

#### 4.Feedforward

类似于KQV的操作

#### 5. transformer encoder整体结构

5.1 字向量与位置编码
$$
X=EmbeddingLookup(X)+PositionalEncoding
\\
X\in R^{batch size*seq.len.*embed.dim.}
$$


5.2 自注意力机制
$$
Q=Linear(X)=X\cdot W_Q \\
K=Linear(X)=X\cdot W_K \\
V=Linear(X)=X\cdot W_V \\
X_{attention}=SelfAttention(Q,K,V)
$$




5.3 残差连接和Layer Normalization
$$
X_{attention}=X+X_{attention}
\\
X_{attention}=LayerNorm(X_{attention})
$$


5.4 Feed forward

是两层线性映射并用激活函数激活，比如ReLU
$$
X_{hidden}=Activate(Linear(Linear(X_{attention})))
$$
第一次把维度拓展成更大的维度

第二次在压缩，形成残差连接



5.5 重复5.3
$$
X_{hidden}=X_{attention}+X_{hidden}\\
X_{hidden}=LayerNorm(X_{hidden})\\
X_{hidden}\in R^{batch size*seq.len.*embed.dim.}
$$
==我们可以在句子开头加一个特殊字符，经过多轮的transformerBlock机制之后，这个特殊字符就含有句子中所有词语的信息，所有的信息向特殊字符的向量汇总，之后就可以进行映射操作==







 <img src="https://n.sinaimg.cn/front/741/w917h624/20190108/4rKs-hrkkwef7008355.jpg" alt="img" style="zoom:67%;" />

论文中编码器有六个，解码器有六个，具体数字可以随意，只是论文这么用

<img src="https://n.sinaimg.cn/front/9/w946h663/20190108/P-K9-hrkkwef7008787.jpg" alt="img" style="zoom:67%;" />

### 3.Bert解读

#### 1.bert语言模型任务：Masked Language Model

是bert与训练任务之一，随机遮盖或替换一句换里面的任意字词，让模型通过上下文理解预测哪一个被遮盖或替换的部分

做**Loss**的时候，只计算被遮盖部分的**Loss**，具体操作为

​	1.随机把一句话的15%的Token替换为以下内容：

​		1）这些token有80几率被替换为[mask]

​		2）10%被替换为任意一个其他Token

​		3）10%原封不动

​	2.之后让模型预测和还原被遮盖掉或者替换掉的部分，模型最终输出的隐藏层的计算结果的维度是：

​	$X_ {hidden}:[batch\_size,seq\_len,embedding\_dim]$

​	我们初始化一个映射层的权重W~vacab~ ：

​	$X_ {vacab}:[embedding\_dim,vocab\_size]$

​	我们用W~vocab~完成隐藏维度到字向量的映射，只要求X~hidden~和W~vocab~的矩阵乘（点积）：

​	$X_ {hidden}W_{vocab}:[batch\_size,seq\_len,vocab\_dim]$之后把上面的结果再vocab_size（最后一个）维度做softmax归一化，使每个字对	应的vocab_size和为1，我们就可以通过vocab_size里概率最大的字来得到模型的预测结果，就可以和我们准备好的Label做损失	（Loss）并反穿梯度了

​	做损失的时候，只计算再第一步里当句中随机遮盖或替换的部分，其余部分不做损失，对于其他部分，模型输出什么，我们并不关心

#### 2.bert语言模型任务二：Next sentence prediction

1. 首先我们拿到属于上下文的一对句子，两个句子，之后在这两端连续的句子里面加一些特殊的token，[cls]和[sep]

   <img src="C:\Users\q1171\AppData\Roaming\Typora\typora-user-images\image-20200909145522197.png" alt="image-20200909145522197" style="zoom:80%;" />

2. 然后准备不属于上下文关系的两句话，比如[cls]my dog is cute [sep] pleguin do not good at fly [sep]

   在实际的过程中，我们让上面两种情况出现的比例为1：1，也就是一般的时间输出的文本属于上下文关系，一半时间不是

3. 我们进行完上述步骤之后，还要随意初始化一个可训练的segment embeddings，见上图中，就是用embedding的信息让模型分开上下句，一半给上句全0的token，下句全1，让模型可以上下句的起始位置

4. 注意力机制就是，让每句话中的每个字对应的向量中，都融入这句话所有字的信息，在最终隐藏层的计算结果里，只要取出[cls]所对应的一条向量，里面就含有整个句子信息





### 4.SUM-QE: a BERT-based Summary Quality Estimation Model

基于bert模型的summary评估系统

SUM-QE能够告知用户自动生成的文本质量和摘要的质量

传统通过ROUGE来评价摘要模型，但ROUGE与摘要的语言质量关联性不大





### 5.基于BERT-PGN模型的中文新闻文本自动摘要生成

关于测试摘要的数据集，可以选用CCF国际自认语言处理与中文计算会议（NLPCC2017），来自于NLPCC2017中文单文档新闻摘要评测数据集，包含训练集新闻文本 49500 篇，测试集新闻文本500 篇。该任务中要求生成的摘要长度不超过 60个字符。

BERT-PGN分为两个阶段，

1. 利用Bert机制获取文章词向量，利用多维语义特征对句子进行打分，将二者拼接生成输入序列，
2. 将得到的输入序列输入到指针生成网络模型，使用coverage机制减少生成重复文字，保留新生成文字的能力，得到新闻摘要

指针生成网络模型结合了指针网络(PointerNetwork,PN) 和基于注意力机制的序列到序列模型，允许通过指针直接指 向生成的单词，也可以从固定的词汇表中生成单词。

### 6.摘要重要性的理论模型

A Simple Theoretical Model of Importance for Summarization

1.abstract：

摘要是有损语义的压缩模型

2.框架

2.1 语义单元也是一条信息，语义单位的频率分布可以被纳入考虑

2.2 冗余

### 7.T5 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer

**Architectures**

1. 原始的Transformer结构表现最好
2. encoder-decoder结构和BERT、GPT的计算量差不多
3. 共享encoder和decoder的参数没有使效果差太多

**Unsupervised objectives**

1. 自编码和自回归的效果差不多
2. 作者推荐选择更短目标序列的目标函数，提高计算效率

**Datasets**

1. 在领域内进行无监督训练可以提升一些任务的效果，但在一个小领域数据上重复训练会降低效果
2. Large、diverse的数据集最香了

**Training strategies**

1. 精调时更新所有参数 > 更新部分参数
2. 在多个任务上预训练之后精调 = 无监督预训练

**Scaling**

1. 在小模型上训练更多数据 < 用少量步数训练更大的模型
2. 从一个预训练模型上精调多个模型后集成 < 分开预训练+精调后集成